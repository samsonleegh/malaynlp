{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:transformers.tokenization_utils_base:Model name 'huseinzol05/xlnet-base-bahasa-cased' not found in model shortcut name list (xlnet-base-cased, xlnet-large-cased). Assuming 'huseinzol05/xlnet-base-bahasa-cased' is a path, a model identifier, or url to a directory containing tokenizer files.\nINFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/huseinzol05/xlnet-base-bahasa-cased/spiece.model from cache at /Users/samsonlee/.cache/torch/transformers/c5ed46a1c7dc1002ab4f2106928fce75836edca5e1988fb9ef5c7b34eadb7a88.69797efcf2cbceb2ff4faaa9fda1b49630bc0a6af197b3bf7709a355149d5f4a\nINFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/huseinzol05/xlnet-base-bahasa-cased/added_tokens.json from cache at None\nINFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/huseinzol05/xlnet-base-bahasa-cased/special_tokens_map.json from cache at /Users/samsonlee/.cache/torch/transformers/cf7e33ce887a48f40b5be1d5be8384cfda70401efab9a70e54cfcb57f342711e.c8bf1a060873efecf794f7e9a64522b72a23da98523221c5634141aab2a46c3b\nINFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/huseinzol05/xlnet-base-bahasa-cased/tokenizer_config.json from cache at /Users/samsonlee/.cache/torch/transformers/2273b57e6470f2754ffd47d77e42c65eda0f1249bc89e04d1dda16d32698ac8a.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\nINFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/huseinzol05/xlnet-base-bahasa-cased/tokenizer.json from cache at None\n"
    }
   ],
   "source": [
    "\n",
    "from transformers import XLNetTokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\n",
    "    'huseinzol05/xlnet-base-bahasa-cased', do_lower_case = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('export_model/vocab-xlnet-base.json') as fopen:\n",
    "    data = json.load(fopen)\n",
    "    \n",
    "LABEL_VOCAB = data['label']\n",
    "TAG_VOCAB = data['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.gfile.GFile('export_model/xlnet-base.pb.quantized', 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = graph.get_tensor_by_name('import/input_ids:0')\n",
    "word_end_mask = graph.get_tensor_by_name('import/word_end_mask:0')\n",
    "charts = graph.get_tensor_by_name('import/charts:0')\n",
    "tags = graph.get_tensor_by_name('import/tags:0')\n",
    "sess = tf.InteractiveSession(graph = graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "BERT_MAX_LEN = 512\n",
    "import numpy as np\n",
    "from parse_nk_xlnet_base import BERT_TOKEN_MAPPING\n",
    "\n",
    "def make_feed_dict_bert(sentences):\n",
    "    all_input_ids = np.zeros((len(sentences), BERT_MAX_LEN), dtype=int)\n",
    "    all_word_end_mask = np.zeros((len(sentences), BERT_MAX_LEN), dtype=int)\n",
    "\n",
    "    subword_max_len = 0\n",
    "    for snum, sentence in enumerate(sentences):\n",
    "        tokens = []\n",
    "        word_end_mask = []\n",
    "\n",
    "        cleaned_words = []\n",
    "        for word in sentence:\n",
    "            word = BERT_TOKEN_MAPPING.get(word, word)\n",
    "            if word == \"n't\" and cleaned_words:\n",
    "                cleaned_words[-1] = cleaned_words[-1] + \"n\"\n",
    "                word = \"'t\"\n",
    "            cleaned_words.append(word)\n",
    "\n",
    "        for word in cleaned_words:\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            for _ in range(len(word_tokens)):\n",
    "                word_end_mask.append(0)\n",
    "            word_end_mask[-1] = 1\n",
    "            tokens.extend(word_tokens)\n",
    "        tokens.append(\"<sep>\")\n",
    "        word_end_mask.append(1)\n",
    "        tokens.append(\"<cls>\")\n",
    "        word_end_mask.append(1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        subword_max_len = max(subword_max_len, len(input_ids))\n",
    "\n",
    "        all_input_ids[snum, :len(input_ids)] = input_ids\n",
    "        all_word_end_mask[snum, :len(word_end_mask)] = word_end_mask\n",
    "\n",
    "    all_input_ids = all_input_ids[:, :subword_max_len]\n",
    "    all_word_end_mask = all_word_end_mask[:, :subword_max_len]\n",
    "    return all_input_ids, all_word_end_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array([[  383,  1096, 21767,    88,   757,  1606, 15738,    24,   198,\n          4049,  2479,  7529,   271,  7644,     9,     4,     3]]),\n array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1]]))"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "s = 'Dr Mahathir menasihati mereka supaya berhenti berehat dan tidur sebentar sekiranya mengantuk ketika memandu.'.split()\n",
    "sentences = [s]\n",
    "i, m = make_feed_dict_bert(sentences)\n",
    "i, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array([[[[ 0.        , -2.7198172 , -2.6694536 , ..., -2.8303301 ,\n           -2.8180645 , -2.3391323 ],\n          [ 0.        , -1.6336582 , -2.2570708 , ..., -1.8680124 ,\n           -1.8989975 , -1.8138791 ],\n          [ 0.        , -0.92996144, -1.759844  , ..., -2.0503466 ,\n           -1.6889832 , -2.0735917 ],\n          ...,\n          [ 0.        , -1.5485951 , -2.700939  , ..., -1.7113425 ,\n           -1.8153486 , -2.475861  ],\n          [ 0.        , -1.9920502 , -2.868455  , ..., -2.0737116 ,\n           -2.0749557 , -2.1500504 ],\n          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n            0.        ,  0.        ]],\n \n         [[ 0.        , -1.5348125 , -1.6977209 , ..., -1.6391273 ,\n           -1.8877805 , -1.9240421 ],\n          [ 0.        , -2.7198172 , -2.6694536 , ..., -2.8303301 ,\n           -2.8180645 , -2.3391323 ],\n          [ 0.        , -1.2483176 , -1.5230591 , ..., -1.9593236 ,\n           -1.7585529 , -2.185865  ],\n          ...,\n          [ 0.        , -1.5352175 , -2.4236279 , ..., -1.632581  ,\n           -2.2270331 , -2.1694708 ],\n          [ 0.        , -1.9555161 , -3.0542095 , ..., -1.8705515 ,\n           -2.2436733 , -2.1166606 ],\n          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n            0.        ,  0.        ]],\n \n         [[ 0.        , -1.378867  , -1.2701701 , ..., -1.9610635 ,\n           -2.0520473 , -1.8978589 ],\n          [ 0.        , -1.680275  , -1.6512715 , ..., -2.0775151 ,\n           -2.0864012 , -1.8973747 ],\n          [ 0.        , -2.7198172 , -2.6694536 , ..., -2.8303301 ,\n           -2.8180645 , -2.3391323 ],\n          ...,\n          [ 0.        , -1.4918888 , -2.1626482 , ..., -1.6310399 ,\n           -2.0053022 , -2.1003406 ],\n          [ 0.        , -2.1005063 , -2.6987987 , ..., -2.023542  ,\n           -2.0924215 , -2.1688035 ],\n          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n            0.        ,  0.        ]],\n \n         ...,\n \n         [[ 0.        , -1.4635464 , -2.1829562 , ..., -1.615948  ,\n           -1.7672431 , -2.1032274 ],\n          [ 0.        , -1.5206008 , -2.3818486 , ..., -1.7387162 ,\n           -2.1617205 , -1.7096847 ],\n          [ 0.        , -1.1144408 , -2.1035688 , ..., -1.6238308 ,\n           -1.6573151 , -1.8735814 ],\n          ...,\n          [ 0.        , -2.7198172 , -2.6694536 , ..., -2.8303301 ,\n           -2.8180645 , -2.3391323 ],\n          [ 0.        , -2.052777  , -3.1766248 , ..., -2.1467993 ,\n           -2.3432305 , -1.9660788 ],\n          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n            0.        ,  0.        ]],\n \n         [[ 0.        , -1.46398   , -1.0499994 , ..., -1.5532544 ,\n           -1.5514951 , -2.0056913 ],\n          [ 0.        , -1.351493  , -1.2043241 , ..., -1.3771442 ,\n           -1.59634   , -1.8271655 ],\n          [ 0.        , -1.3427124 , -1.1448368 , ..., -1.5485283 ,\n           -1.3728169 , -2.0868003 ],\n          ...,\n          [ 0.        , -1.5234002 , -1.6034418 , ..., -1.6600919 ,\n           -1.8297768 , -2.0394907 ],\n          [ 0.        , -2.719817  , -2.6694531 , ..., -2.8303297 ,\n           -2.8180645 , -2.3391323 ],\n          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n            0.        ,  0.        ]],\n \n         [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n            0.        ,  0.        ],\n          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n            0.        ,  0.        ],\n          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n            0.        ,  0.        ],\n          ...,\n          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n            0.        ,  0.        ],\n          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n            0.        ,  0.        ],\n          [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n            0.        ,  0.        ]]]], dtype=float32),\n array([[ 0,  4,  6,  8, 14, 11,  8,  5, 11,  8,  5,  3,  5,  4, 16,  1]]))"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "charts_val, tags_val = sess.run((charts, tags), {input_ids: i, word_end_mask: m})\n",
    "charts_val, tags_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for snum, sentence in enumerate(sentences):\n",
    "    chart_size = len(sentence) + 1\n",
    "    chart = charts_val[snum,:chart_size,:chart_size,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'chart_decoder.pyx'"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "import wget\n",
    "url_chart_decoder = 'https://raw.githubusercontent.com/michaeljohns2/self-attentive-parser/michaeljohns2-support-tf2-patch/benepar/chart_decoder.pyx'\n",
    "wget.download(url_chart_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chart_decoder_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(7.619638919830322,\n array([ 0,  0,  0,  1,  2,  2,  2,  3,  3,  4,  4,  5,  5,  6,  6,  7,  7,\n         8,  8,  9,  9, 10, 10, 11, 11, 12, 13]),\n array([14,  2,  1,  2, 14, 13,  3, 13,  4, 13,  5, 13,  6, 13,  7, 13,  8,\n        13,  9, 13, 10, 13, 11, 13, 12, 13, 14]),\n array([ 3, 10,  0, 10,  0,  7,  0,  0,  5,  7,  0,  7,  0,  0,  0,  0,  0,\n         7,  0,  0,  0,  2,  0,  3, 12,  0,  0]))"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "chart_decoder_py.decode(chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "PTB_TOKEN_ESCAPE = {u\"(\": u\"-LRB-\",\n",
    "    u\")\": u\"-RRB-\",\n",
    "    u\"{\": u\"-LCB-\",\n",
    "    u\"}\": u\"-RCB-\",\n",
    "    u\"[\": u\"-LSB-\",\n",
    "    u\"]\": u\"-RSB-\"}\n",
    "\n",
    "\n",
    "def make_nltk_tree(sentence, tags, score, p_i, p_j, p_label):\n",
    "\n",
    "    # Python 2 doesn't support \"nonlocal\", so wrap idx in a list\n",
    "    idx_cell = [-1]\n",
    "    def make_tree():\n",
    "        idx_cell[0] += 1\n",
    "        idx = idx_cell[0]\n",
    "        i, j, label_idx = p_i[idx], p_j[idx], p_label[idx]\n",
    "        label = LABEL_VOCAB[label_idx]\n",
    "        if (i + 1) >= j:\n",
    "            word = sentence[i]\n",
    "            tag = TAG_VOCAB[tags[i]]\n",
    "            tag = PTB_TOKEN_ESCAPE.get(tag, tag)\n",
    "            word = PTB_TOKEN_ESCAPE.get(word, word)\n",
    "            tree = Tree(tag, [word])\n",
    "            for sublabel in label[::-1]:\n",
    "                tree = Tree(sublabel, [tree])\n",
    "            return [tree]\n",
    "        else:\n",
    "            left_trees = make_tree()\n",
    "            right_trees = make_tree()\n",
    "            children = left_trees + right_trees\n",
    "            if label:\n",
    "                tree = Tree(label[-1], children)\n",
    "                for sublabel in reversed(label[:-1]):\n",
    "                    tree = Tree(sublabel, [tree])\n",
    "                return [tree]\n",
    "            else:\n",
    "                return children\n",
    "\n",
    "    tree = make_tree()[0]\n",
    "    tree.score = score\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(S\n  (NP-SBJ (<START> Dr) (NP-SBJ (NN Mahathir)))\n  (VP\n    (NNP menasihati)\n    (NP (VB mereka))\n    (VP\n      (PRP supaya)\n      (VP\n        (CC berhenti)\n        (VB berehat)\n        (JJ dan)\n        (VP\n          (CC tidur)\n          (VB sebentar)\n          (SBAR\n            (JJ sekiranya)\n            (S (FRAG (NP (IN mengantuk))) (JJ ketika)))))))\n  (NN memandu.))\n"
    }
   ],
   "source": [
    "tree = make_nltk_tree(s, tags_val[0], *chart_decoder_py.decode(chart))\n",
    "print(str(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_str_tree(sentence, tags, score, p_i, p_j, p_label):\n",
    "    idx_cell = [-1]\n",
    "    def make_str():\n",
    "        idx_cell[0] += 1\n",
    "        idx = idx_cell[0]\n",
    "        i, j, label_idx = p_i[idx], p_j[idx], p_label[idx]\n",
    "        label = LABEL_VOCAB[label_idx]\n",
    "        if (i + 1) >= j:\n",
    "            word = sentence[i]\n",
    "            tag = TAG_VOCAB[tags[i]]\n",
    "            tag = PTB_TOKEN_ESCAPE.get(tag, tag)\n",
    "            word = PTB_TOKEN_ESCAPE.get(word, word)\n",
    "            s = u\"({} {})\".format(tag, word)\n",
    "        else:\n",
    "            children = []\n",
    "            while ((idx_cell[0] + 1) < len(p_i)\n",
    "                and i <= p_i[idx_cell[0] + 1]\n",
    "                and p_j[idx_cell[0] + 1] <= j):\n",
    "                children.append(make_str())\n",
    "\n",
    "            s = u\" \".join(children)\n",
    "            \n",
    "        for sublabel in reversed(label):\n",
    "            s = u\"({} {})\".format(sublabel, s)\n",
    "        return s\n",
    "    return make_str()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'(S (NP-SBJ (<START> Dr) (NP-SBJ (NN Mahathir))) (VP (NNP menasihati) (NP (VB mereka)) (VP (PRP supaya) (VP (CC berhenti) (VB berehat) (JJ dan) (VP (CC tidur) (VB sebentar) (SBAR (JJ sekiranya) (S (FRAG (NP (IN mengantuk))) (JJ ketika))))))) (NN memandu.))'"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "make_str_tree(s, tags_val[0], *chart_decoder_py.decode(chart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'(S\\n  (NP-SBJ (<START> Dr) (NP-SBJ (NN Mahathir)))\\n  (VP\\n    (NNP menasihati)\\n    (NP (VB mereka))\\n    (VP\\n      (PRP supaya)\\n      (VP\\n        (CC berhenti)\\n        (VB berehat)\\n        (JJ dan)\\n        (VP\\n          (CC tidur)\\n          (VB sebentar)\\n          (SBAR\\n            (JJ sekiranya)\\n            (S (FRAG (NP (IN mengantuk))) (JJ ketika)))))))\\n  (NN memandu.))'"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "str(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'(S<br>  (NP-SBJ (<START> Dr) (NP-SBJ (NN Mahathir)))<br>  (VP<br>    (NNP menasihati)<br>    (NP (VB mereka))<br>    (VP<br>      (PRP supaya)<br>      (VP<br>        (CC berhenti)<br>        (VB berehat)<br>        (JJ dan)<br>        (VP<br>          (CC tidur)<br>          (VB sebentar)<br>          (SBAR<br>            (JJ sekiranya)<br>            (S (FRAG (NP (IN mengantuk))) (JJ ketika)))))))<br>  (NN memandu.))'"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "import re\n",
    "re.sub('\\n', ' <br>', str(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bitc7a971407c2947b894ce474cf086911d",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}